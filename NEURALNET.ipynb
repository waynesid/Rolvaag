{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the team related datasets into the notebook and save them as dataframes\n",
    "def country_df(csv):\n",
    "  df = pd.read_csv(csv)\n",
    "  df.sort_values('HomeTeam', inplace=True)\n",
    "  df.reset_index(inplace=True)\n",
    "  df.drop(\"index\", axis =1 , inplace=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "df_france = country_df('FootballData/F1.csv')\n",
    "df_spain  = country_df('FootballData/SP1.csv')\n",
    "df_england  = country_df('FootballData/E0.csv')\n",
    "df_championship  = country_df('FootballData/E1.csv')\n",
    "#df_finland  = country_df('/content/drive/MyDrive/FootballData/FIN.csv')\n",
    "#df_germany = country_df('FootballData/D1.csv')\n",
    "df_england_22 = country_df('FootballData/E0_22.csv')\n",
    "df_spa_22 = country_df('FootballData/SP1_22.csv')\n",
    "#df_jpn = country_df('FootballData/JPN.csv')\n",
    "df_ita = country_df('FootballData/I1.csv')\n",
    "df_E1 = country_df('FootballData/E1.csv')\n",
    "df_E2 = country_df('FootballData/E2.csv')\n",
    "df_E3 = country_df('FootballData/E3.csv')\n",
    "df_EC = country_df('FootballData/EC.csv')\n",
    "df_F2 = country_df('FootballData/F2.csv')\n",
    "df_I2 = country_df('FootballData/I2.csv')\n",
    "df_dataE0 = country_df('FootballData/data (9)/E0.csv')\n",
    "df_dataE1 = country_df('FootballData/data (9)/E1.csv')\n",
    "df_dataE2 = country_df('FootballData/data (9)/E2.csv')\n",
    "df_dataE3 = country_df('FootballData/data (9)/E3.csv')\n",
    "df_dataEC = country_df('FootballData/data (9)/EC.csv')\n",
    "df_dataF2 = country_df('FootballData/data (9)/F2.csv')\n",
    "df_dataG1 = country_df('FootballData/data (9)/G1.csv')\n",
    "df_dataN1 = country_df('FootballData/data (9)/N1.csv')\n",
    "df_dataP1 = country_df('FootballData/data (9)/P1.csv')\n",
    "df_dataSC0 = country_df('FootballData/data (9)/SC0.csv')\n",
    "df_dataSC1 = country_df('FootballData/data (9)/SC1.csv')\n",
    "df_dataSC2 = country_df('FootballData/data (9)/SC2.csv')\n",
    "df_dataSC3 = country_df('FootballData/data (9)/SC3.csv')\n",
    "df_dataSP2 = country_df('FootballData/data (9)/SP2.csv')\n",
    "df_dataSP1 = country_df('FootballData/data (9)/SP1.csv')\n",
    "df_dataT1 = country_df('FootballData/data (9)/T1.csv')\n",
    "\n",
    "df_data8E0 = country_df('FootballData/data (8)/E0.csv')\n",
    "df_data8E1 = country_df('FootballData/data (8)/E1.csv')\n",
    "df_data8E2 = country_df('FootballData/data (8)/E2.csv')\n",
    "df_data8E3 = country_df('FootballData/data (8)/E3.csv')\n",
    "df_data8EC = country_df('FootballData/data (8)/EC.csv')\n",
    "#df_data8F2 = country_df('FootballData/data (8)/F2.csv')\n",
    "df_data8G1 = country_df('FootballData/data (8)/G1.csv')\n",
    "#df_data8N1 = country_df('FootballData/data (8)/N1.csv')\n",
    "#df_data8P1 = country_df('FootballData/data (8)/P1.csv')\n",
    "df_data8SC0 = country_df('FootballData/data (8)/SC0.csv')\n",
    "df_data8SC1 = country_df('FootballData/data (8)/SC1.csv')\n",
    "df_data8SC2 = country_df('FootballData/data (8)/SC2.csv')\n",
    "df_data8SC3 = country_df('FootballData/data (8)/SC3.csv')\n",
    "#df_data8SP2 = country_df('FootballData/data (8)/SP2.csv')\n",
    "df_data8SP1 = country_df('FootballData/data (8)/SP1.csv')\n",
    "df_data8T1 = country_df('FootballData/data (8)/T1.csv')\n",
    "df_B1 = country_df('FootballData/B1.csv')\n",
    "\n",
    "\n",
    "\n",
    "data = pd.concat([df_france, df_spain, df_england, df_championship, df_england_22, df_spa_22, df_ita,df_E1,df_E2,df_E3,df_EC,df_F2,df_I2,df_dataE0,df_dataE1,df_dataE2,df_dataE3,df_dataEC,df_dataF2,df_dataG1,df_dataN1,df_dataP1,df_dataSC0,df_dataSC1,df_dataSC2,df_dataSC3,df_dataSP1,df_dataSP2,df_dataT1,\n",
    "                  df_data8E0,df_data8E1,df_data8E2,df_data8E3,df_data8EC,df_data8G1,df_data8SC0,df_data8SC1,df_data8SC2,df_data8SC3,df_data8SP1,df_data8T1], sort = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"FTHG\",\"FTAG\",\"FTR\",\"B365H\",\"B365D\",\"B365A\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for rows with NaN values\n",
    "mask = data.isnull().any(axis=1)\n",
    "\n",
    "# Delete rows using the mask\n",
    "data = data[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FTR'] = data['FTR'].map({'H':0,'D':1,'A':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding .values for the neural network\n",
    "X=data.drop(\"FTR\", axis=1).values\n",
    "Y=data[\"FTR\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to train and test dataset.\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetworkClassificationModel(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(NeuralNetworkClassificationModel,self).__init__()\n",
    "        self.input_layer    = nn.Linear(input_dim,16)\n",
    "        self.hidden_layer1  = nn.Linear(16,64)\n",
    "        self.hidden_layer2  = nn.Linear(64,256)\n",
    "        self.hidden_layer3  = nn.Linear(256,256)\n",
    "        self.output_layer   = nn.Linear(256,output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)  # Add this line\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out =  self.dropout(self.relu(self.input_layer(x)))\n",
    "        out =  self.dropout(self.relu(self.hidden_layer1(out)))\n",
    "        out =  self.dropout(self.relu(self.hidden_layer2(out)))\n",
    "        out =  self.dropout(self.relu(self.hidden_layer3(out)))\n",
    "        out =  self.output_layer(out)\n",
    "        out =  self.softmax(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim  = 5\n",
    "output_dim = 3\n",
    "model = NeuralNetworkClassificationModel(input_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define batch size.\n",
    "batch_size =1000\n",
    "\n",
    "# Create Tensor datasets.\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders.\n",
    "dataloader_train = DataLoader(train_data, shuffle=False, batch_size=batch_size)\n",
    "dataloader_test = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters to consider ; momentum=int, nesterov=boolean,decay_rate=(int) learning_rate/epochs -->these are in the optimizer field\n",
    "#adding batch size in training\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, optimizer, criterion, dataloader_train, dataloader_test, num_epochs, train_losses, test_losses):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for X_train, y_train in dataloader_train:\n",
    "            # Clear out the gradients from the last step (loss.backward()).\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward feed.\n",
    "            output_train = model(X_train)\n",
    "\n",
    "            # Calculate the loss.\n",
    "            loss_train = criterion(output_train, y_train)\n",
    "\n",
    "            # Backward propagation: calculate gradients.\n",
    "            loss_train.backward()\n",
    "\n",
    "            # Update the weights.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the loss.\n",
    "            import numpy as np\n",
    "\n",
    "            # Record the loss.\n",
    "            train_losses = np.append(train_losses, loss_train.item())\n",
    "\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in dataloader_test:\n",
    "                output_test = model(X_test)\n",
    "                loss_test = criterion(output_test, y_test)\n",
    "\n",
    "                # Record the loss.\n",
    "                test_losses = np.append(loss_test, loss_test.item())\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {np.mean(train_losses):.7f}, Test Loss: {np.mean(test_losses):.7f}\")\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network(model, optimizer, criterion, dataloader_train, dataloader_test, num_epochs, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = []\n",
    "predictions_test =  []\n",
    "with torch.no_grad():\n",
    "    predictions_train = model(X_train)\n",
    "    predictions_test = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_multiclass(pred_arr,original_arr):\n",
    "    if len(pred_arr)!=len(original_arr):\n",
    "        return False\n",
    "    pred_arr = pred_arr.numpy()\n",
    "    original_arr = original_arr.numpy()\n",
    "    final_pred= []\n",
    "    # we will get something like this in the pred_arr [32.1680,12.9350,-58.4877]\n",
    "    # so will be taking the index of that argument which has the highest value here 32.1680 which corresponds to 0th index\n",
    "    for i in range(len(pred_arr)):\n",
    "        final_pred.append(np.argmax(pred_arr[i]))\n",
    "    final_pred = np.array(final_pred)\n",
    "    count = 0\n",
    "    #here we are doing a simple comparison between the predicted_arr and the original_arr to get the final accuracy\n",
    "    for i in range(len(original_arr)):\n",
    "        if final_pred[i] == original_arr[i]:\n",
    "            count+=1\n",
    "    return count/len(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = get_accuracy_multiclass(predictions_train,y_train)\n",
    "test_acc  = get_accuracy_multiclass(predictions_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Accuracy: {train_acc}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[\"FTHG\",\"FTAG\",\"HTHG\",\"HTAG\",\"Form\",\"B365H\",\"B365D\",\"B365A\"]\n",
    "X_new = np.array([[1.08,1.08,1.00,1.60,2.54,3.30,2.96]])\n",
    "X_net = torch.FloatTensor(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# During inference or evaluation, apply softmax to obtain class probabilities\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model(X_net)\n",
    "\n",
    "  # prediction = preprocessing.scale(prediction)\n",
    "\n",
    "    print(\"Prediction of Games: {}\".format(prediction))\n",
    "\n",
    "\n",
    "# Assuming prediction is a 1D tensor with 3 elements\n",
    "prediction_np = prediction.numpy().flatten()\n",
    "\n",
    "# Calculate the differences\n",
    "diff_12 = prediction_np[1] - prediction_np[0]\n",
    "diff_23 = prediction_np[2] - prediction_np[1]\n",
    "diff_31 = prediction_np[0] - prediction_np[2]\n",
    "\n",
    "# Find the maximum and minimum\n",
    "max_val = np.max(prediction_np)\n",
    "min_val = np.min(prediction_np)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Difference 1-2': [diff_12],\n",
    "    'Difference 2-3': [diff_23],\n",
    "    'Difference 3-1': [diff_31],\n",
    "    'Maximum': [max_val],\n",
    "    'Minimum': [min_val]\n",
    "})\n",
    "\n",
    "print(df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "# Create Pickle file from the Neural network Model\n",
    "#with open('neuralnet.pickle', 'wb') as dump_var:\n",
    " #   pickle.dump(data, dump_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickled_model = pickle.load(open('model.pkl', 'rb'))\n",
    "#pickled_model(X_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rolvaag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
